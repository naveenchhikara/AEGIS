---
phase: 05-foundation-and-migration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - docker-compose.dev.yml
  - .env.example
  - .env
  - .dockerignore
  - Dockerfile
autonomous: true

must_haves:
  truths:
    - PostgreSQL container runs locally and accepts connections
    - Docker Compose starts PostgreSQL without errors
    - .env.example uses clearly-fake placeholder values (CHANGE_ME_IN_PRODUCTION)
    - S3 IAM policy grants PutObject and GetObject ONLY (no DeleteObject)
  artifacts:
    - path: docker-compose.yml
      provides: PostgreSQL service definition (no Redis in Phase 5)
      min_lines: 30
    - path: .env.example
      provides: Template for all required environment variables with safe placeholders
      min_lines: 20
    - path: Dockerfile
      provides: Multi-stage Next.js build for production
      min_lines: 30
  key_links:
    - from: Next.js app
      to: PostgreSQL
      via: DATABASE_URL connection string
      pattern: "postgresql://.*:5432/aegis"
    - from: Server actions
      to: AWS S3
      via: AWS SDK with credentials
      pattern: "process\\.env\\.AWS_.*"
---

<objective>
Establish core infrastructure for multi-tenant backend: local PostgreSQL database, AWS S3 bucket for evidence file storage (immutable — no delete), and Docker Compose orchestration for development and production deployment.

Purpose: Create the foundation infrastructure that all subsequent backend features depend on. Without database and storage, no data persistence is possible.

Output: Running local development environment with PostgreSQL 16 and AWS S3 bucket configured in Mumbai region. No Redis in Phase 5 (deferred to Phase 8 for email queue — Decision D3).

**Key decisions incorporated:**

- D3: No Redis in Phase 5 (Better Auth uses DB sessions)
- D10: Remove S3 DeleteObject from IAM policy (evidence is immutable — Skeptic S6)
- DE10: Backup/DR replication must target ap-south-2 (Hyderabad), never outside India
  </objective>

<execution_context>
@/Users/admin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/admin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/research/ARCHITECTURE.md
@.planning/research/PITFALLS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Docker Compose infrastructure (PostgreSQL only)</name>
  <files>
    docker-compose.yml
    docker-compose.dev.yml
    .dockerignore
  </files>
  <action>
    Create Docker Compose configuration with PostgreSQL service only (no Redis — Decision D3):

    **PostgreSQL 16:**
    - Image: postgres:16-alpine
    - Database: aegis
    - Port: 5432 (internal), 5433 (host) to avoid conflict with local postgres
    - Volume: postgres_data for persistence
    - Healthcheck: pg_isready every 10s
    - Environment: POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB from .env

    **Next.js App (production only, not dev):**
    - Build from Dockerfile
    - Port: 3000
    - Depends on: PostgreSQL (healthy)
    - Environment: All vars from .env

    Add .dockerignore to exclude node_modules, .next, .git, .planning:
    ```
    node_modules
    .next
    .git
    .planning
    .env*
    !.env.example
    README.md
    docker-compose*.yml
    ```

    Use docker-compose.dev.yml override for development (no Next.js container, just Postgres).

    **IMPORTANT:** Do NOT include Redis. Better Auth uses DB sessions by default. Redis will be added in Phase 8 for email queue.

  </action>
  <verify>
    docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d
    docker-compose ps (PostgreSQL service "healthy")
    psql postgresql://aegis:aegis_dev_password@localhost:5433/aegis -c "SELECT version();"
  </verify>
  <done>
    Docker Compose starts PostgreSQL, service passes health check, can connect via psql CLI.
  </done>
</task>

<task type="auto">
  <name>Task 2: Configure AWS S3 bucket for evidence storage (immutable)</name>
  <files>
    .env.example
    scripts/setup-s3.sh
  </files>
  <action>
    Create S3 bucket configuration in Mumbai region (ap-south-1) for evidence file storage:

    **Bucket configuration:**
    - Name: aegis-evidence-{env} (e.g., aegis-evidence-dev, aegis-evidence-prod)
    - Region: ap-south-1 (Mumbai) — RBI data localization requirement
    - Versioning: Enabled
    - Encryption: SSE-S3 (server-side encryption)
    - Block public access: All enabled (private bucket)
    - Lifecycle policy: Delete incomplete multipart uploads after 7 days

    **CORS configuration:**
    ```json
    {
      "CORSRules": [{
        "AllowedOrigins": ["http://localhost:3000", "https://aegis-audit.com"],
        "AllowedMethods": ["PUT", "GET"],
        "AllowedHeaders": ["*"],
        "MaxAgeSeconds": 3000
      }]
    }
    ```

    **IAM User:**
    - Name: aegis-s3-user
    - Policy: **PutObject and GetObject ONLY** (NO DeleteObject — Decision D10, Skeptic S6)
    - Evidence files are immutable once uploaded. If wrong file uploaded, upload a replacement — never delete the original.
    - Soft-delete via database `deleted_at` timestamp if needed in future.
    - Generate access key and secret key

    **DR Note (DE10):** If cross-region replication is configured for disaster recovery, target MUST be ap-south-2 (Hyderabad). Never replicate to a non-India AWS region.

    Create scripts/setup-s3.sh with AWS CLI commands to automate bucket creation.

    Add to .env.example (with SAFE placeholder values):
    ```
    # AWS S3 Evidence Storage (Mumbai region - RBI data localization)
    AWS_REGION=ap-south-1
    AWS_ACCESS_KEY_ID=CHANGE_ME_IN_PRODUCTION
    AWS_SECRET_ACCESS_KEY=CHANGE_ME_IN_PRODUCTION
    S3_BUCKET_NAME=aegis-evidence-dev
    ```

  </action>
  <verify>
    aws s3 ls s3://aegis-evidence-dev (shows empty bucket)
    aws s3api get-bucket-cors --bucket aegis-evidence-dev (shows CORS config)
    aws s3api get-bucket-encryption --bucket aegis-evidence-dev (shows SSE-S3)
    # Verify DeleteObject is denied:
    echo "test" > /tmp/test.txt && aws s3 cp /tmp/test.txt s3://aegis-evidence-dev/test.txt
    aws s3 rm s3://aegis-evidence-dev/test.txt (should be DENIED by IAM policy)
  </verify>
  <done>
    S3 bucket exists in Mumbai region with versioning, encryption, CORS, and IAM user with PutObject + GetObject only (no DeleteObject).
  </done>
</task>

<task type="auto">
  <name>Task 3: Create production Dockerfile and .env.example</name>
  <files>
    Dockerfile
    .dockerignore
    .env.example
    .env
  </files>
  <action>
    Create multi-stage Dockerfile optimized for Next.js 16 standalone output:

    **Stage 1: Dependencies**
    - FROM node:22-alpine AS deps
    - Install pnpm via corepack
    - Copy package.json, pnpm-lock.yaml
    - RUN pnpm install --frozen-lockfile

    **Stage 2: Builder**
    - FROM node:22-alpine AS builder
    - Copy node_modules from deps stage
    - Copy all source code
    - Set NEXT_TELEMETRY_DISABLED=1
    - RUN pnpm prisma generate
    - RUN pnpm build (creates .next/standalone)

    **Stage 3: Runner (production)**
    - FROM node:22-alpine AS runner
    - Create non-root user (nextjs:nodejs)
    - Copy .next/standalone, .next/static, public from builder
    - Set NODE_ENV=production
    - EXPOSE 3000
    - CMD ["node", "server.js"]
    - HEALTHCHECK: curl http://localhost:3000/api/health every 30s

    **Create complete .env.example with SAFE placeholder values (Skeptic Finding 9):**
    ```
    # Database (PostgreSQL 16)
    POSTGRES_USER=aegis
    POSTGRES_PASSWORD=CHANGE_ME_IN_PRODUCTION
    POSTGRES_DB=aegis
    DATABASE_URL=postgresql://aegis:CHANGE_ME_IN_PRODUCTION@localhost:5433/aegis

    # Authentication (Better Auth)
    BETTER_AUTH_SECRET=CHANGE_ME_GENERATE_WITH_openssl_rand_base64_32
    BETTER_AUTH_URL=http://localhost:3000

    # AWS S3 Evidence Storage (Mumbai region)
    AWS_REGION=ap-south-1
    AWS_ACCESS_KEY_ID=CHANGE_ME_IN_PRODUCTION
    AWS_SECRET_ACCESS_KEY=CHANGE_ME_IN_PRODUCTION
    S3_BUCKET_NAME=aegis-evidence-dev

    # Application
    NODE_ENV=development
    NEXT_PUBLIC_APP_URL=http://localhost:3000
    ```

    **IMPORTANT:** All secrets use `CHANGE_ME_IN_PRODUCTION` or `CHANGE_ME_*` — never use real-looking values like `password` or `secret123`.

    Also create a `.env` file for local development with actual dev values (gitignored).

  </action>
  <verify>
    docker build -t aegis:latest .
    docker images | grep aegis (image size <500MB)
    # Verify .env.example has no real credentials:
    grep -c "CHANGE_ME" .env.example (should return 4+)
    grep -c "password" .env.example (should return 0)
  </verify>
  <done>
    Dockerfile builds successfully, .env.example uses safe placeholders, .env has local dev values.
  </done>
</task>

</tasks>

<verification>
**Infrastructure validation:**
1. docker-compose up -d starts PostgreSQL without errors
2. PostgreSQL accepts connections on port 5433
3. S3 bucket accessible via AWS CLI with IAM credentials
4. S3 DeleteObject is denied by IAM policy
5. Docker image builds without errors
6. All environment variables documented in .env.example with safe placeholders
7. No Redis container (deferred to Phase 8)

**Security validation (Skeptic review):**

- .env.example has CHANGE_ME placeholders (no real-looking credentials)
- S3 IAM policy: PutObject + GetObject only (evidence immutability)
- DR note documented for ap-south-2 only
  </verification>

<success_criteria>

1. Developer runs `docker-compose up -d` and sees healthy PostgreSQL
2. psql connection string from .env works
3. S3 bucket accepts test file upload and denies delete via AWS CLI
4. Production Docker image builds and runs Next.js server
5. .env.example contains NO real-looking credentials
   </success_criteria>

<output>
After completion, create `.planning/phases/05-foundation-and-migration/05-01-SUMMARY.md`
</output>
